{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/project/\"\n",
    "# had to rename the \"model\" folder to \"models\"\n",
    "sz=224\n",
    "arch = resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz), test_name=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/160306600.jpg',\n",
       " 'test/210761939.jpg',\n",
       " 'test/5.jpg',\n",
       " 'test/173701358.jpg',\n",
       " 'test/197303393.jpg',\n",
       " 'test/2.jpg',\n",
       " 'test/1.jpg',\n",
       " 'test/210836813.jpg',\n",
       " 'test/4.jpg',\n",
       " 'test/187172553.jpg',\n",
       " 'test/7.jpg',\n",
       " 'test/3.jpg',\n",
       " 'test/195556035.jpg',\n",
       " 'test/166493158.jpg',\n",
       " 'test/186159803.jpg',\n",
       " 'test/160173742.jpg',\n",
       " 'test/185543547.jpg',\n",
       " 'test/164932508.jpg',\n",
       " 'test/181160404.jpg',\n",
       " 'test/6.jpg',\n",
       " 'test/183813036.jpg',\n",
       " 'test/195562508.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save test set for use with caffe2 \n",
    "(img, label) = next(iter(data.test_dl))\n",
    "test_img_numpy = img.cpu().numpy()\n",
    "test_labels_numpy = label.cpu().numpy()\n",
    "np.save(\"test_img_numpy\", test_img_numpy)\n",
    "np.save(\"test_label_numpy\", test_labels_numpy)\n",
    "\n",
    "data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x300 at 0x7F7AF0A636D8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.open(PATH + \"test/160306600.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Load the model with fastai and check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.conv_learner.caffe2_batch_norm_compat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"sleeves_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "  )\n",
       "  (9): Flatten(\n",
       "  )\n",
       "  (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (11): Dropout(p=0.25)\n",
       "  (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (15): Dropout(p=0.5)\n",
       "  (16): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (17): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds = learn.predict(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00087, 0.00029, 0.15039, 0.84845],\n",
       "       [0.99802, 0.00076, 0.00002, 0.00121],\n",
       "       [0.99906, 0.00025, 0.00001, 0.00068],\n",
       "       [0.02508, 0.87104, 0.08766, 0.01622],\n",
       "       [0.97537, 0.02162, 0.00037, 0.00264],\n",
       "       [0.99859, 0.00053, 0.00003, 0.00085],\n",
       "       [0.01338, 0.9761 , 0.00588, 0.00464],\n",
       "       [0.99846, 0.0003 , 0.00002, 0.00122],\n",
       "       [0.9948 , 0.00282, 0.00027, 0.0021 ],\n",
       "       [0.00235, 0.00667, 0.87351, 0.11747],\n",
       "       [0.02373, 0.97369, 0.00109, 0.00149],\n",
       "       [0.99892, 0.00026, 0.00002, 0.0008 ],\n",
       "       [0.0137 , 0.00432, 0.15233, 0.82965],\n",
       "       [0.96586, 0.02985, 0.00236, 0.00194],\n",
       "       [0.01003, 0.01191, 0.2424 , 0.73566],\n",
       "       [0.08843, 0.66529, 0.08015, 0.16613],\n",
       "       [0.07464, 0.92336, 0.00042, 0.00158],\n",
       "       [0.00008, 0.00008, 0.02065, 0.97919],\n",
       "       [0.05247, 0.86872, 0.06157, 0.01723],\n",
       "       [0.00001, 0.00002, 0.00634, 0.99362],\n",
       "       [0.00008, 0.00004, 0.03924, 0.96064],\n",
       "       [0.00308, 0.002  , 0.26308, 0.73184]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.exp(log_preds); probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the predictions seem alright. now let's extract the pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.models.model\n",
    "model = model.cpu()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Variable(img).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_preds = model(model_input).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = np.exp(test_log_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00087, 0.00029, 0.15039, 0.84845],\n",
       "       [0.99802, 0.00076, 0.00002, 0.00121],\n",
       "       [0.99906, 0.00025, 0.00001, 0.00068],\n",
       "       [0.02508, 0.87104, 0.08766, 0.01622],\n",
       "       [0.97537, 0.02162, 0.00037, 0.00264],\n",
       "       [0.99859, 0.00053, 0.00003, 0.00085],\n",
       "       [0.01338, 0.9761 , 0.00588, 0.00464],\n",
       "       [0.99846, 0.0003 , 0.00002, 0.00122],\n",
       "       [0.9948 , 0.00282, 0.00027, 0.0021 ],\n",
       "       [0.00235, 0.00667, 0.87351, 0.11747],\n",
       "       [0.02373, 0.97369, 0.00109, 0.00149],\n",
       "       [0.99892, 0.00026, 0.00002, 0.0008 ],\n",
       "       [0.0137 , 0.00432, 0.15233, 0.82965],\n",
       "       [0.96586, 0.02985, 0.00236, 0.00194],\n",
       "       [0.01003, 0.01191, 0.2424 , 0.73566],\n",
       "       [0.08843, 0.66529, 0.08015, 0.16613],\n",
       "       [0.07464, 0.92336, 0.00042, 0.00158],\n",
       "       [0.00008, 0.00008, 0.02065, 0.97919],\n",
       "       [0.05247, 0.86872, 0.06157, 0.01723],\n",
       "       [0.00001, 0.00002, 0.00634, 0.99362],\n",
       "       [0.00008, 0.00004, 0.03924, 0.96064],\n",
       "       [0.00308, 0.002  , 0.26308, 0.73184]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(test_probs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "amazing, so the pytorch model produces the same output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we try to export the model now to ONNX, it will export correctly, but will fail to run in caffe2 lateron (batchnorm shape issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%1 : Float(1, 3, 224, 224)\n",
      "      %2 : Float(64, 3, 7, 7)\n",
      "      %3 : Float(64)\n",
      "      %4 : Float(64)\n",
      "      %5 : Float(64)\n",
      "      %6 : Float(64)\n",
      "      %7 : Float(64, 64, 3, 3)\n",
      "      %8 : Float(64)\n",
      "      %9 : Float(64)\n",
      "      %10 : Float(64)\n",
      "      %11 : Float(64)\n",
      "      %12 : Float(64, 64, 3, 3)\n",
      "      %13 : Float(64)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64, 64, 3, 3)\n",
      "      %18 : Float(64)\n",
      "      %19 : Float(64)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64, 64, 3, 3)\n",
      "      %23 : Float(64)\n",
      "      %24 : Float(64)\n",
      "      %25 : Float(64)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64, 64, 3, 3)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Float(64)\n",
      "      %31 : Float(64)\n",
      "      %32 : Float(64, 64, 3, 3)\n",
      "      %33 : Float(64)\n",
      "      %34 : Float(64)\n",
      "      %35 : Float(64)\n",
      "      %36 : Float(64)\n",
      "      %37 : Float(128, 64, 3, 3)\n",
      "      %38 : Float(128)\n",
      "      %39 : Float(128)\n",
      "      %40 : Float(128)\n",
      "      %41 : Float(128)\n",
      "      %42 : Float(128, 128, 3, 3)\n",
      "      %43 : Float(128)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128, 64, 1, 1)\n",
      "      %48 : Float(128)\n",
      "      %49 : Float(128)\n",
      "      %50 : Float(128)\n",
      "      %51 : Float(128)\n",
      "      %52 : Float(128, 128, 3, 3)\n",
      "      %53 : Float(128)\n",
      "      %54 : Float(128)\n",
      "      %55 : Float(128)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128, 128, 3, 3)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Float(128)\n",
      "      %61 : Float(128)\n",
      "      %62 : Float(128, 128, 3, 3)\n",
      "      %63 : Float(128)\n",
      "      %64 : Float(128)\n",
      "      %65 : Float(128)\n",
      "      %66 : Float(128)\n",
      "      %67 : Float(128, 128, 3, 3)\n",
      "      %68 : Float(128)\n",
      "      %69 : Float(128)\n",
      "      %70 : Float(128)\n",
      "      %71 : Float(128)\n",
      "      %72 : Float(128, 128, 3, 3)\n",
      "      %73 : Float(128)\n",
      "      %74 : Float(128)\n",
      "      %75 : Float(128)\n",
      "      %76 : Float(128)\n",
      "      %77 : Float(128, 128, 3, 3)\n",
      "      %78 : Float(128)\n",
      "      %79 : Float(128)\n",
      "      %80 : Float(128)\n",
      "      %81 : Float(128)\n",
      "      %82 : Float(256, 128, 3, 3)\n",
      "      %83 : Float(256)\n",
      "      %84 : Float(256)\n",
      "      %85 : Float(256)\n",
      "      %86 : Float(256)\n",
      "      %87 : Float(256, 256, 3, 3)\n",
      "      %88 : Float(256)\n",
      "      %89 : Float(256)\n",
      "      %90 : Float(256)\n",
      "      %91 : Float(256)\n",
      "      %92 : Float(256, 128, 1, 1)\n",
      "      %93 : Float(256)\n",
      "      %94 : Float(256)\n",
      "      %95 : Float(256)\n",
      "      %96 : Float(256)\n",
      "      %97 : Float(256, 256, 3, 3)\n",
      "      %98 : Float(256)\n",
      "      %99 : Float(256)\n",
      "      %100 : Float(256)\n",
      "      %101 : Float(256)\n",
      "      %102 : Float(256, 256, 3, 3)\n",
      "      %103 : Float(256)\n",
      "      %104 : Float(256)\n",
      "      %105 : Float(256)\n",
      "      %106 : Float(256)\n",
      "      %107 : Float(256, 256, 3, 3)\n",
      "      %108 : Float(256)\n",
      "      %109 : Float(256)\n",
      "      %110 : Float(256)\n",
      "      %111 : Float(256)\n",
      "      %112 : Float(256, 256, 3, 3)\n",
      "      %113 : Float(256)\n",
      "      %114 : Float(256)\n",
      "      %115 : Float(256)\n",
      "      %116 : Float(256)\n",
      "      %117 : Float(256, 256, 3, 3)\n",
      "      %118 : Float(256)\n",
      "      %119 : Float(256)\n",
      "      %120 : Float(256)\n",
      "      %121 : Float(256)\n",
      "      %122 : Float(256, 256, 3, 3)\n",
      "      %123 : Float(256)\n",
      "      %124 : Float(256)\n",
      "      %125 : Float(256)\n",
      "      %126 : Float(256)\n",
      "      %127 : Float(256, 256, 3, 3)\n",
      "      %128 : Float(256)\n",
      "      %129 : Float(256)\n",
      "      %130 : Float(256)\n",
      "      %131 : Float(256)\n",
      "      %132 : Float(256, 256, 3, 3)\n",
      "      %133 : Float(256)\n",
      "      %134 : Float(256)\n",
      "      %135 : Float(256)\n",
      "      %136 : Float(256)\n",
      "      %137 : Float(256, 256, 3, 3)\n",
      "      %138 : Float(256)\n",
      "      %139 : Float(256)\n",
      "      %140 : Float(256)\n",
      "      %141 : Float(256)\n",
      "      %142 : Float(256, 256, 3, 3)\n",
      "      %143 : Float(256)\n",
      "      %144 : Float(256)\n",
      "      %145 : Float(256)\n",
      "      %146 : Float(256)\n",
      "      %147 : Float(512, 256, 3, 3)\n",
      "      %148 : Float(512)\n",
      "      %149 : Float(512)\n",
      "      %150 : Float(512)\n",
      "      %151 : Float(512)\n",
      "      %152 : Float(512, 512, 3, 3)\n",
      "      %153 : Float(512)\n",
      "      %154 : Float(512)\n",
      "      %155 : Float(512)\n",
      "      %156 : Float(512)\n",
      "      %157 : Float(512, 256, 1, 1)\n",
      "      %158 : Float(512)\n",
      "      %159 : Float(512)\n",
      "      %160 : Float(512)\n",
      "      %161 : Float(512)\n",
      "      %162 : Float(512, 512, 3, 3)\n",
      "      %163 : Float(512)\n",
      "      %164 : Float(512)\n",
      "      %165 : Float(512)\n",
      "      %166 : Float(512)\n",
      "      %167 : Float(512, 512, 3, 3)\n",
      "      %168 : Float(512)\n",
      "      %169 : Float(512)\n",
      "      %170 : Float(512)\n",
      "      %171 : Float(512)\n",
      "      %172 : Float(512, 512, 3, 3)\n",
      "      %173 : Float(512)\n",
      "      %174 : Float(512)\n",
      "      %175 : Float(512)\n",
      "      %176 : Float(512)\n",
      "      %177 : Float(512, 512, 3, 3)\n",
      "      %178 : Float(512)\n",
      "      %179 : Float(512)\n",
      "      %180 : Float(512)\n",
      "      %181 : Float(512)\n",
      "      %182 : Float(1024)\n",
      "      %183 : Float(1024)\n",
      "      %184 : Float(1024)\n",
      "      %185 : Float(1024)\n",
      "      %186 : Float(512, 1024)\n",
      "      %187 : Float(512)\n",
      "      %188 : Float(512)\n",
      "      %189 : Float(512)\n",
      "      %190 : Float(512)\n",
      "      %191 : Float(512)\n",
      "      %192 : Float(4, 512)\n",
      "      %193 : Float(4)) {\n",
      "  %195 : Float(1, 64, 112, 112) = Conv[kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3], dilations=[1, 1], group=1](%1, %2), uses = [%196.i0], scope: Sequential/Conv2d[0];\n",
      "  %197 : Float(1, 64, 112, 112) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%195, %3, %4, %5, %6), uses = [[%198.i0]], scope: Sequential/BatchNorm2d[1];\n",
      "  %198 : Float(1, 64, 112, 112) = Relu(%197), uses = [%199.i0], scope: Sequential/ReLU[2];\n",
      "  %199 : Float(1, 64, 56, 56) = MaxPool[kernel_shape=[3, 3], pads=[1, 1], strides=[2, 2]](%198), uses = [%201.i0, %209.i1], scope: Sequential/MaxPool2d[3];\n",
      "  %201 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%199, %7), uses = [%202.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %203 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%201, %8, %9, %10, %11), uses = [[%204.i0]], scope: Sequential/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %204 : Float(1, 64, 56, 56) = Relu(%203), uses = [%206.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/ReLU[relu];\n",
      "  %206 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%204, %12), uses = [%207.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %208 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%206, %13, %14, %15, %16), uses = [[%209.i0]], scope: Sequential/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %209 : Float(1, 64, 56, 56) = Add(%208, %199), uses = [%210.i0], scope: Sequential/Sequential[4]/BasicBlock[0];\n",
      "  %210 : Float(1, 64, 56, 56) = Relu(%209), uses = [%212.i0, %220.i1], scope: Sequential/Sequential[4]/BasicBlock[0]/ReLU[relu];\n",
      "  %212 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%210, %17), uses = [%213.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %214 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%212, %18, %19, %20, %21), uses = [[%215.i0]], scope: Sequential/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %215 : Float(1, 64, 56, 56) = Relu(%214), uses = [%217.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/ReLU[relu];\n",
      "  %217 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%215, %22), uses = [%218.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %219 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%217, %23, %24, %25, %26), uses = [[%220.i0]], scope: Sequential/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %220 : Float(1, 64, 56, 56) = Add(%219, %210), uses = [%221.i0], scope: Sequential/Sequential[4]/BasicBlock[1];\n",
      "  %221 : Float(1, 64, 56, 56) = Relu(%220), uses = [%223.i0, %231.i1], scope: Sequential/Sequential[4]/BasicBlock[1]/ReLU[relu];\n",
      "  %223 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%221, %27), uses = [%224.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %225 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%223, %28, %29, %30, %31), uses = [[%226.i0]], scope: Sequential/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %226 : Float(1, 64, 56, 56) = Relu(%225), uses = [%228.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/ReLU[relu];\n",
      "  %228 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%226, %32), uses = [%229.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %230 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%228, %33, %34, %35, %36), uses = [[%231.i0]], scope: Sequential/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %231 : Float(1, 64, 56, 56) = Add(%230, %221), uses = [%232.i0], scope: Sequential/Sequential[4]/BasicBlock[2];\n",
      "  %232 : Float(1, 64, 56, 56) = Relu(%231), uses = [%234.i0, %243.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/ReLU[relu];\n",
      "  %234 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%232, %37), uses = [%235.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %236 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%234, %38, %39, %40, %41), uses = [[%237.i0]], scope: Sequential/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %237 : Float(1, 128, 28, 28) = Relu(%236), uses = [%239.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/ReLU[relu];\n",
      "  %239 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%237, %42), uses = [%240.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %241 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%239, %43, %44, %45, %46), uses = [[%246.i0]], scope: Sequential/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %243 : Float(1, 128, 28, 28) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%232, %47), uses = [%244.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %245 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%243, %48, %49, %50, %51), uses = [[%246.i1]], scope: Sequential/Sequential[5]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %246 : Float(1, 128, 28, 28) = Add(%241, %245), uses = [%247.i0], scope: Sequential/Sequential[5]/BasicBlock[0];\n",
      "  %247 : Float(1, 128, 28, 28) = Relu(%246), uses = [%249.i0, %257.i1], scope: Sequential/Sequential[5]/BasicBlock[0]/ReLU[relu];\n",
      "  %249 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%247, %52), uses = [%250.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %251 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%249, %53, %54, %55, %56), uses = [[%252.i0]], scope: Sequential/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %252 : Float(1, 128, 28, 28) = Relu(%251), uses = [%254.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/ReLU[relu];\n",
      "  %254 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%252, %57), uses = [%255.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %256 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%254, %58, %59, %60, %61), uses = [[%257.i0]], scope: Sequential/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %257 : Float(1, 128, 28, 28) = Add(%256, %247), uses = [%258.i0], scope: Sequential/Sequential[5]/BasicBlock[1];\n",
      "  %258 : Float(1, 128, 28, 28) = Relu(%257), uses = [%260.i0, %268.i1], scope: Sequential/Sequential[5]/BasicBlock[1]/ReLU[relu];\n",
      "  %260 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%258, %62), uses = [%261.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %262 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%260, %63, %64, %65, %66), uses = [[%263.i0]], scope: Sequential/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %263 : Float(1, 128, 28, 28) = Relu(%262), uses = [%265.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/ReLU[relu];\n",
      "  %265 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%263, %67), uses = [%266.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %267 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%265, %68, %69, %70, %71), uses = [[%268.i0]], scope: Sequential/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %268 : Float(1, 128, 28, 28) = Add(%267, %258), uses = [%269.i0], scope: Sequential/Sequential[5]/BasicBlock[2];\n",
      "  %269 : Float(1, 128, 28, 28) = Relu(%268), uses = [%271.i0, %279.i1], scope: Sequential/Sequential[5]/BasicBlock[2]/ReLU[relu];\n",
      "  %271 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%269, %72), uses = [%272.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/Conv2d[conv1];\n",
      "  %273 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%271, %73, %74, %75, %76), uses = [[%274.i0]], scope: Sequential/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn1];\n",
      "  %274 : Float(1, 128, 28, 28) = Relu(%273), uses = [%276.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/ReLU[relu];\n",
      "  %276 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%274, %77), uses = [%277.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/Conv2d[conv2];\n",
      "  %278 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%276, %78, %79, %80, %81), uses = [[%279.i0]], scope: Sequential/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn2];\n",
      "  %279 : Float(1, 128, 28, 28) = Add(%278, %269), uses = [%280.i0], scope: Sequential/Sequential[5]/BasicBlock[3];\n",
      "  %280 : Float(1, 128, 28, 28) = Relu(%279), uses = [%282.i0, %291.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/ReLU[relu];\n",
      "  %282 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%280, %82), uses = [%283.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %284 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%282, %83, %84, %85, %86), uses = [[%285.i0]], scope: Sequential/Sequential[6]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %285 : Float(1, 256, 14, 14) = Relu(%284), uses = [%287.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/ReLU[relu];\n",
      "  %287 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%285, %87), uses = [%288.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %289 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%287, %88, %89, %90, %91), uses = [[%294.i0]], scope: Sequential/Sequential[6]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %291 : Float(1, 256, 14, 14) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%280, %92), uses = [%292.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %293 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%291, %93, %94, %95, %96), uses = [[%294.i1]], scope: Sequential/Sequential[6]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %294 : Float(1, 256, 14, 14) = Add(%289, %293), uses = [%295.i0], scope: Sequential/Sequential[6]/BasicBlock[0];\n",
      "  %295 : Float(1, 256, 14, 14) = Relu(%294), uses = [%297.i0, %305.i1], scope: Sequential/Sequential[6]/BasicBlock[0]/ReLU[relu];\n",
      "  %297 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%295, %97), uses = [%298.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %299 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%297, %98, %99, %100, %101), uses = [[%300.i0]], scope: Sequential/Sequential[6]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %300 : Float(1, 256, 14, 14) = Relu(%299), uses = [%302.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/ReLU[relu];\n",
      "  %302 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%300, %102), uses = [%303.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %304 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%302, %103, %104, %105, %106), uses = [[%305.i0]], scope: Sequential/Sequential[6]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %305 : Float(1, 256, 14, 14) = Add(%304, %295), uses = [%306.i0], scope: Sequential/Sequential[6]/BasicBlock[1];\n",
      "  %306 : Float(1, 256, 14, 14) = Relu(%305), uses = [%308.i0, %316.i1], scope: Sequential/Sequential[6]/BasicBlock[1]/ReLU[relu];\n",
      "  %308 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%306, %107), uses = [%309.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %310 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%308, %108, %109, %110, %111), uses = [[%311.i0]], scope: Sequential/Sequential[6]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %311 : Float(1, 256, 14, 14) = Relu(%310), uses = [%313.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/ReLU[relu];\n",
      "  %313 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%311, %112), uses = [%314.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %315 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%313, %113, %114, %115, %116), uses = [[%316.i0]], scope: Sequential/Sequential[6]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %316 : Float(1, 256, 14, 14) = Add(%315, %306), uses = [%317.i0], scope: Sequential/Sequential[6]/BasicBlock[2];\n",
      "  %317 : Float(1, 256, 14, 14) = Relu(%316), uses = [%319.i0, %327.i1], scope: Sequential/Sequential[6]/BasicBlock[2]/ReLU[relu];\n",
      "  %319 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%317, %117), uses = [%320.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/Conv2d[conv1];\n",
      "  %321 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%319, %118, %119, %120, %121), uses = [[%322.i0]], scope: Sequential/Sequential[6]/BasicBlock[3]/BatchNorm2d[bn1];\n",
      "  %322 : Float(1, 256, 14, 14) = Relu(%321), uses = [%324.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/ReLU[relu];\n",
      "  %324 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%322, %122), uses = [%325.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/Conv2d[conv2];\n",
      "  %326 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%324, %123, %124, %125, %126), uses = [[%327.i0]], scope: Sequential/Sequential[6]/BasicBlock[3]/BatchNorm2d[bn2];\n",
      "  %327 : Float(1, 256, 14, 14) = Add(%326, %317), uses = [%328.i0], scope: Sequential/Sequential[6]/BasicBlock[3];\n",
      "  %328 : Float(1, 256, 14, 14) = Relu(%327), uses = [%330.i0, %338.i1], scope: Sequential/Sequential[6]/BasicBlock[3]/ReLU[relu];\n",
      "  %330 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%328, %127), uses = [%331.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/Conv2d[conv1];\n",
      "  %332 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%330, %128, %129, %130, %131), uses = [[%333.i0]], scope: Sequential/Sequential[6]/BasicBlock[4]/BatchNorm2d[bn1];\n",
      "  %333 : Float(1, 256, 14, 14) = Relu(%332), uses = [%335.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/ReLU[relu];\n",
      "  %335 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%333, %132), uses = [%336.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/Conv2d[conv2];\n",
      "  %337 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%335, %133, %134, %135, %136), uses = [[%338.i0]], scope: Sequential/Sequential[6]/BasicBlock[4]/BatchNorm2d[bn2];\n",
      "  %338 : Float(1, 256, 14, 14) = Add(%337, %328), uses = [%339.i0], scope: Sequential/Sequential[6]/BasicBlock[4];\n",
      "  %339 : Float(1, 256, 14, 14) = Relu(%338), uses = [%341.i0, %349.i1], scope: Sequential/Sequential[6]/BasicBlock[4]/ReLU[relu];\n",
      "  %341 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%339, %137), uses = [%342.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/Conv2d[conv1];\n",
      "  %343 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%341, %138, %139, %140, %141), uses = [[%344.i0]], scope: Sequential/Sequential[6]/BasicBlock[5]/BatchNorm2d[bn1];\n",
      "  %344 : Float(1, 256, 14, 14) = Relu(%343), uses = [%346.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/ReLU[relu];\n",
      "  %346 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%344, %142), uses = [%347.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/Conv2d[conv2];\n",
      "  %348 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%346, %143, %144, %145, %146), uses = [[%349.i0]], scope: Sequential/Sequential[6]/BasicBlock[5]/BatchNorm2d[bn2];\n",
      "  %349 : Float(1, 256, 14, 14) = Add(%348, %339), uses = [%350.i0], scope: Sequential/Sequential[6]/BasicBlock[5];\n",
      "  %350 : Float(1, 256, 14, 14) = Relu(%349), uses = [%352.i0, %361.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/ReLU[relu];\n",
      "  %352 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%350, %147), uses = [%353.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %354 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%352, %148, %149, %150, %151), uses = [[%355.i0]], scope: Sequential/Sequential[7]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %355 : Float(1, 512, 7, 7) = Relu(%354), uses = [%357.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/ReLU[relu];\n",
      "  %357 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%355, %152), uses = [%358.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %359 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%357, %153, %154, %155, %156), uses = [[%364.i0]], scope: Sequential/Sequential[7]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %361 : Float(1, 512, 7, 7) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%350, %157), uses = [%362.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %363 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%361, %158, %159, %160, %161), uses = [[%364.i1]], scope: Sequential/Sequential[7]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %364 : Float(1, 512, 7, 7) = Add(%359, %363), uses = [%365.i0], scope: Sequential/Sequential[7]/BasicBlock[0];\n",
      "  %365 : Float(1, 512, 7, 7) = Relu(%364), uses = [%367.i0, %375.i1], scope: Sequential/Sequential[7]/BasicBlock[0]/ReLU[relu];\n",
      "  %367 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%365, %162), uses = [%368.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %369 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%367, %163, %164, %165, %166), uses = [[%370.i0]], scope: Sequential/Sequential[7]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %370 : Float(1, 512, 7, 7) = Relu(%369), uses = [%372.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/ReLU[relu];\n",
      "  %372 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%370, %167), uses = [%373.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %374 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%372, %168, %169, %170, %171), uses = [[%375.i0]], scope: Sequential/Sequential[7]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %375 : Float(1, 512, 7, 7) = Add(%374, %365), uses = [%376.i0], scope: Sequential/Sequential[7]/BasicBlock[1];\n",
      "  %376 : Float(1, 512, 7, 7) = Relu(%375), uses = [%378.i0, %386.i1], scope: Sequential/Sequential[7]/BasicBlock[1]/ReLU[relu];\n",
      "  %378 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%376, %172), uses = [%379.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %380 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%378, %173, %174, %175, %176), uses = [[%381.i0]], scope: Sequential/Sequential[7]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %381 : Float(1, 512, 7, 7) = Relu(%380), uses = [%383.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/ReLU[relu];\n",
      "  %383 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%381, %177), uses = [%384.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %385 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%383, %178, %179, %180, %181), uses = [[%386.i0]], scope: Sequential/Sequential[7]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %386 : Float(1, 512, 7, 7) = Add(%385, %376), uses = [%387.i0], scope: Sequential/Sequential[7]/BasicBlock[2];\n",
      "  %387 : Float(1, 512, 7, 7) = Relu(%386), uses = [%388.i0, %389.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/ReLU[relu];\n",
      "  %388 : Float(1, 512, 1, 1) = MaxPool[kernel_shape=[7, 7], pads=[0, 0], strides=[7, 7]](%387), uses = [%390.i0], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %389 : Float(1, 512, 1, 1) = AveragePool[kernel_shape=[7, 7], pads=[0, 0], strides=[7, 7]](%387), uses = [%390.i1], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %390 : Float(1, 1024, 1, 1) = Concat[axis=1](%388, %389), uses = [%391.i0], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %391 : Float(1, 1024) = Reshape[shape=[1, -1]](%390), uses = [%392.i0], scope: Sequential/Flatten[9];\n",
      "  %393 : Float(1, 1024) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%391, %182, %183, %184, %185), uses = [[%394.i0]], scope: Sequential/BatchNorm1d[10];\n",
      "  %395 : Float(1, 1024), %396 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.25](%393), uses = [[%399.i0], []], scope: Sequential/Dropout[11];\n",
      "  %399 : Float(1, 512) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%395, %186, %187), uses = [%400.i0], scope: Sequential/Linear[12];\n",
      "  %400 : Float(1, 512) = Relu(%399), uses = [%401.i0], scope: Sequential/ReLU[13];\n",
      "  %402 : Float(1, 512) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%400, %188, %189, %190, %191), uses = [[%403.i0]], scope: Sequential/BatchNorm1d[14];\n",
      "  %404 : Float(1, 512), %405 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%402), uses = [[%408.i0], []], scope: Sequential/Dropout[15];\n",
      "  %408 : Float(1, 4) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%404, %192, %193), uses = [%409.i0], scope: Sequential/Linear[16];\n",
      "  %409 : Float(1, 4) = Softmax[axis=1](%408), uses = [%410.i0], scope: Sequential/LogSoftmax[17];\n",
      "  %410 : Float(1, 4) = Log(%409), uses = [%0.i0], scope: Sequential/LogSoftmax[17];\n",
      "  return (%410);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from torch documentation\n",
    "from torch.autograd import Variable\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "\n",
    "dummy_input = Variable(torch.randn(1, 3, 224, 224)).cpu()\n",
    "\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"sleeves_no_batchnorm_fix.proto\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - activate batchnorm compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.conv_learner.caffe2_batch_norm_compat = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch=resnet34\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz), test_name=\"test\")\n",
    "learn = ConvLearner.pretrained(arch, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice the new \"BatchNormExpand\" and \"BatchNormContract\" layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "  )\n",
       "  (9): Flatten(\n",
       "  )\n",
       "  (10): BatchNormExpand(\n",
       "  )\n",
       "  (11): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (12): BatchNormContract(\n",
       "  )\n",
       "  (13): Dropout(p=0.25)\n",
       "  (14): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (15): ReLU()\n",
       "  (16): BatchNormExpand(\n",
       "  )\n",
       "  (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (18): BatchNormContract(\n",
       "  )\n",
       "  (19): Dropout(p=0.5)\n",
       "  (20): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (21): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we  to load the weights now, it will fail, as the new layers change the structure of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'unexpected key \"10.weight\" in state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4356636a32d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sleeves_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Software/Sources/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Sources/fastai/courses/dl1/fastai/torch_imports.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(m, p)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 raise KeyError('unexpected key \"{}\" in state_dict'\n\u001b[0;32m--> 522\u001b[0;31m                                .format(name))\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'unexpected key \"10.weight\" in state_dict'"
     ]
    }
   ],
   "source": [
    "learn.load(\"sleeves_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets extract the model, fix the naming issues and load the weights. If you train the model with this fork of fastai, you dont need to do this (actually it would break the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.models.model\n",
    "model = model.cpu()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(PATH + \"models/sleeves_v1.h5\", map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '1.weight', '1.bias', '1.running_mean', '1.running_var', '4.0.conv1.weight', '4.0.bn1.weight', '4.0.bn1.bias', '4.0.bn1.running_mean', '4.0.bn1.running_var', '4.0.conv2.weight', '4.0.bn2.weight', '4.0.bn2.bias', '4.0.bn2.running_mean', '4.0.bn2.running_var', '4.1.conv1.weight', '4.1.bn1.weight', '4.1.bn1.bias', '4.1.bn1.running_mean', '4.1.bn1.running_var', '4.1.conv2.weight', '4.1.bn2.weight', '4.1.bn2.bias', '4.1.bn2.running_mean', '4.1.bn2.running_var', '4.2.conv1.weight', '4.2.bn1.weight', '4.2.bn1.bias', '4.2.bn1.running_mean', '4.2.bn1.running_var', '4.2.conv2.weight', '4.2.bn2.weight', '4.2.bn2.bias', '4.2.bn2.running_mean', '4.2.bn2.running_var', '5.0.conv1.weight', '5.0.bn1.weight', '5.0.bn1.bias', '5.0.bn1.running_mean', '5.0.bn1.running_var', '5.0.conv2.weight', '5.0.bn2.weight', '5.0.bn2.bias', '5.0.bn2.running_mean', '5.0.bn2.running_var', '5.0.downsample.0.weight', '5.0.downsample.1.weight', '5.0.downsample.1.bias', '5.0.downsample.1.running_mean', '5.0.downsample.1.running_var', '5.1.conv1.weight', '5.1.bn1.weight', '5.1.bn1.bias', '5.1.bn1.running_mean', '5.1.bn1.running_var', '5.1.conv2.weight', '5.1.bn2.weight', '5.1.bn2.bias', '5.1.bn2.running_mean', '5.1.bn2.running_var', '5.2.conv1.weight', '5.2.bn1.weight', '5.2.bn1.bias', '5.2.bn1.running_mean', '5.2.bn1.running_var', '5.2.conv2.weight', '5.2.bn2.weight', '5.2.bn2.bias', '5.2.bn2.running_mean', '5.2.bn2.running_var', '5.3.conv1.weight', '5.3.bn1.weight', '5.3.bn1.bias', '5.3.bn1.running_mean', '5.3.bn1.running_var', '5.3.conv2.weight', '5.3.bn2.weight', '5.3.bn2.bias', '5.3.bn2.running_mean', '5.3.bn2.running_var', '6.0.conv1.weight', '6.0.bn1.weight', '6.0.bn1.bias', '6.0.bn1.running_mean', '6.0.bn1.running_var', '6.0.conv2.weight', '6.0.bn2.weight', '6.0.bn2.bias', '6.0.bn2.running_mean', '6.0.bn2.running_var', '6.0.downsample.0.weight', '6.0.downsample.1.weight', '6.0.downsample.1.bias', '6.0.downsample.1.running_mean', '6.0.downsample.1.running_var', '6.1.conv1.weight', '6.1.bn1.weight', '6.1.bn1.bias', '6.1.bn1.running_mean', '6.1.bn1.running_var', '6.1.conv2.weight', '6.1.bn2.weight', '6.1.bn2.bias', '6.1.bn2.running_mean', '6.1.bn2.running_var', '6.2.conv1.weight', '6.2.bn1.weight', '6.2.bn1.bias', '6.2.bn1.running_mean', '6.2.bn1.running_var', '6.2.conv2.weight', '6.2.bn2.weight', '6.2.bn2.bias', '6.2.bn2.running_mean', '6.2.bn2.running_var', '6.3.conv1.weight', '6.3.bn1.weight', '6.3.bn1.bias', '6.3.bn1.running_mean', '6.3.bn1.running_var', '6.3.conv2.weight', '6.3.bn2.weight', '6.3.bn2.bias', '6.3.bn2.running_mean', '6.3.bn2.running_var', '6.4.conv1.weight', '6.4.bn1.weight', '6.4.bn1.bias', '6.4.bn1.running_mean', '6.4.bn1.running_var', '6.4.conv2.weight', '6.4.bn2.weight', '6.4.bn2.bias', '6.4.bn2.running_mean', '6.4.bn2.running_var', '6.5.conv1.weight', '6.5.bn1.weight', '6.5.bn1.bias', '6.5.bn1.running_mean', '6.5.bn1.running_var', '6.5.conv2.weight', '6.5.bn2.weight', '6.5.bn2.bias', '6.5.bn2.running_mean', '6.5.bn2.running_var', '7.0.conv1.weight', '7.0.bn1.weight', '7.0.bn1.bias', '7.0.bn1.running_mean', '7.0.bn1.running_var', '7.0.conv2.weight', '7.0.bn2.weight', '7.0.bn2.bias', '7.0.bn2.running_mean', '7.0.bn2.running_var', '7.0.downsample.0.weight', '7.0.downsample.1.weight', '7.0.downsample.1.bias', '7.0.downsample.1.running_mean', '7.0.downsample.1.running_var', '7.1.conv1.weight', '7.1.bn1.weight', '7.1.bn1.bias', '7.1.bn1.running_mean', '7.1.bn1.running_var', '7.1.conv2.weight', '7.1.bn2.weight', '7.1.bn2.bias', '7.1.bn2.running_mean', '7.1.bn2.running_var', '7.2.conv1.weight', '7.2.bn1.weight', '7.2.bn1.bias', '7.2.bn1.running_mean', '7.2.bn1.running_var', '7.2.conv2.weight', '7.2.bn2.weight', '7.2.bn2.bias', '7.2.bn2.running_mean', '7.2.bn2.running_var', '10.weight', '10.bias', '10.running_mean', '10.running_var', '12.weight', '12.bias', '14.weight', '14.bias', '14.running_mean', '14.running_var', '16.weight', '16.bias'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keymap = {\n",
    "    \"10\":\"11\",\n",
    "    \"11\":\"13\",\n",
    "    \"12\":\"14\",\n",
    "    \"14\":\"17\",\n",
    "    \"15\":\"19\",\n",
    "    \"16\":\"20\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_key(old_key):\n",
    "    prefix = old_key[:2]\n",
    "    suffix = old_key[2:]\n",
    "    return keymap[prefix] + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = OrderedDict((map_key(old_key) if old_key[:2] in keymap.keys() else old_key, v) for old_key, v in state_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '1.weight', '1.bias', '1.running_mean', '1.running_var', '4.0.conv1.weight', '4.0.bn1.weight', '4.0.bn1.bias', '4.0.bn1.running_mean', '4.0.bn1.running_var', '4.0.conv2.weight', '4.0.bn2.weight', '4.0.bn2.bias', '4.0.bn2.running_mean', '4.0.bn2.running_var', '4.1.conv1.weight', '4.1.bn1.weight', '4.1.bn1.bias', '4.1.bn1.running_mean', '4.1.bn1.running_var', '4.1.conv2.weight', '4.1.bn2.weight', '4.1.bn2.bias', '4.1.bn2.running_mean', '4.1.bn2.running_var', '4.2.conv1.weight', '4.2.bn1.weight', '4.2.bn1.bias', '4.2.bn1.running_mean', '4.2.bn1.running_var', '4.2.conv2.weight', '4.2.bn2.weight', '4.2.bn2.bias', '4.2.bn2.running_mean', '4.2.bn2.running_var', '5.0.conv1.weight', '5.0.bn1.weight', '5.0.bn1.bias', '5.0.bn1.running_mean', '5.0.bn1.running_var', '5.0.conv2.weight', '5.0.bn2.weight', '5.0.bn2.bias', '5.0.bn2.running_mean', '5.0.bn2.running_var', '5.0.downsample.0.weight', '5.0.downsample.1.weight', '5.0.downsample.1.bias', '5.0.downsample.1.running_mean', '5.0.downsample.1.running_var', '5.1.conv1.weight', '5.1.bn1.weight', '5.1.bn1.bias', '5.1.bn1.running_mean', '5.1.bn1.running_var', '5.1.conv2.weight', '5.1.bn2.weight', '5.1.bn2.bias', '5.1.bn2.running_mean', '5.1.bn2.running_var', '5.2.conv1.weight', '5.2.bn1.weight', '5.2.bn1.bias', '5.2.bn1.running_mean', '5.2.bn1.running_var', '5.2.conv2.weight', '5.2.bn2.weight', '5.2.bn2.bias', '5.2.bn2.running_mean', '5.2.bn2.running_var', '5.3.conv1.weight', '5.3.bn1.weight', '5.3.bn1.bias', '5.3.bn1.running_mean', '5.3.bn1.running_var', '5.3.conv2.weight', '5.3.bn2.weight', '5.3.bn2.bias', '5.3.bn2.running_mean', '5.3.bn2.running_var', '6.0.conv1.weight', '6.0.bn1.weight', '6.0.bn1.bias', '6.0.bn1.running_mean', '6.0.bn1.running_var', '6.0.conv2.weight', '6.0.bn2.weight', '6.0.bn2.bias', '6.0.bn2.running_mean', '6.0.bn2.running_var', '6.0.downsample.0.weight', '6.0.downsample.1.weight', '6.0.downsample.1.bias', '6.0.downsample.1.running_mean', '6.0.downsample.1.running_var', '6.1.conv1.weight', '6.1.bn1.weight', '6.1.bn1.bias', '6.1.bn1.running_mean', '6.1.bn1.running_var', '6.1.conv2.weight', '6.1.bn2.weight', '6.1.bn2.bias', '6.1.bn2.running_mean', '6.1.bn2.running_var', '6.2.conv1.weight', '6.2.bn1.weight', '6.2.bn1.bias', '6.2.bn1.running_mean', '6.2.bn1.running_var', '6.2.conv2.weight', '6.2.bn2.weight', '6.2.bn2.bias', '6.2.bn2.running_mean', '6.2.bn2.running_var', '6.3.conv1.weight', '6.3.bn1.weight', '6.3.bn1.bias', '6.3.bn1.running_mean', '6.3.bn1.running_var', '6.3.conv2.weight', '6.3.bn2.weight', '6.3.bn2.bias', '6.3.bn2.running_mean', '6.3.bn2.running_var', '6.4.conv1.weight', '6.4.bn1.weight', '6.4.bn1.bias', '6.4.bn1.running_mean', '6.4.bn1.running_var', '6.4.conv2.weight', '6.4.bn2.weight', '6.4.bn2.bias', '6.4.bn2.running_mean', '6.4.bn2.running_var', '6.5.conv1.weight', '6.5.bn1.weight', '6.5.bn1.bias', '6.5.bn1.running_mean', '6.5.bn1.running_var', '6.5.conv2.weight', '6.5.bn2.weight', '6.5.bn2.bias', '6.5.bn2.running_mean', '6.5.bn2.running_var', '7.0.conv1.weight', '7.0.bn1.weight', '7.0.bn1.bias', '7.0.bn1.running_mean', '7.0.bn1.running_var', '7.0.conv2.weight', '7.0.bn2.weight', '7.0.bn2.bias', '7.0.bn2.running_mean', '7.0.bn2.running_var', '7.0.downsample.0.weight', '7.0.downsample.1.weight', '7.0.downsample.1.bias', '7.0.downsample.1.running_mean', '7.0.downsample.1.running_var', '7.1.conv1.weight', '7.1.bn1.weight', '7.1.bn1.bias', '7.1.bn1.running_mean', '7.1.bn1.running_var', '7.1.conv2.weight', '7.1.bn2.weight', '7.1.bn2.bias', '7.1.bn2.running_mean', '7.1.bn2.running_var', '7.2.conv1.weight', '7.2.bn1.weight', '7.2.bn1.bias', '7.2.bn1.running_mean', '7.2.bn1.running_var', '7.2.conv2.weight', '7.2.bn2.weight', '7.2.bn2.bias', '7.2.bn2.running_mean', '7.2.bn2.running_var', '11.weight', '11.bias', '11.running_mean', '11.running_var', '14.weight', '14.bias', '17.weight', '17.bias', '17.running_mean', '17.running_var', '20.weight', '20.bias'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that the keys have been renamed\n",
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great the weights load now correctly. lets check the predictions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00087, 0.00029, 0.15039, 0.84845],\n",
       "       [0.99802, 0.00076, 0.00002, 0.00121],\n",
       "       [0.99906, 0.00025, 0.00001, 0.00068],\n",
       "       [0.02508, 0.87104, 0.08766, 0.01622],\n",
       "       [0.97537, 0.02162, 0.00037, 0.00264],\n",
       "       [0.99859, 0.00053, 0.00003, 0.00085],\n",
       "       [0.01338, 0.9761 , 0.00588, 0.00464],\n",
       "       [0.99846, 0.0003 , 0.00002, 0.00122],\n",
       "       [0.9948 , 0.00282, 0.00027, 0.0021 ],\n",
       "       [0.00235, 0.00667, 0.87351, 0.11747],\n",
       "       [0.02373, 0.97369, 0.00109, 0.00149],\n",
       "       [0.99892, 0.00026, 0.00002, 0.0008 ],\n",
       "       [0.0137 , 0.00432, 0.15233, 0.82965],\n",
       "       [0.96586, 0.02985, 0.00236, 0.00194],\n",
       "       [0.01003, 0.01191, 0.2424 , 0.73566],\n",
       "       [0.08843, 0.66529, 0.08015, 0.16613],\n",
       "       [0.07464, 0.92336, 0.00042, 0.00158],\n",
       "       [0.00008, 0.00008, 0.02065, 0.97919],\n",
       "       [0.05247, 0.86872, 0.06157, 0.01723],\n",
       "       [0.00001, 0.00002, 0.00634, 0.99362],\n",
       "       [0.00008, 0.00004, 0.03924, 0.96064],\n",
       "       [0.00308, 0.002  , 0.26308, 0.73184]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img, label) = next(iter(data.test_dl))\n",
    "\n",
    "model_input = Variable(img).cpu()\n",
    "test_log_preds = model(model_input).data.cpu().numpy()\n",
    "test_probs = np.exp(test_log_preds);test_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exactly the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a precaution, if somewhere in the code torch.Tensor is called, it will be a cpu tensor \n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we can export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%1 : Float(1, 3, 224, 224)\n",
      "      %2 : Float(64, 3, 7, 7)\n",
      "      %3 : Float(64)\n",
      "      %4 : Float(64)\n",
      "      %5 : Float(64)\n",
      "      %6 : Float(64)\n",
      "      %7 : Float(64, 64, 3, 3)\n",
      "      %8 : Float(64)\n",
      "      %9 : Float(64)\n",
      "      %10 : Float(64)\n",
      "      %11 : Float(64)\n",
      "      %12 : Float(64, 64, 3, 3)\n",
      "      %13 : Float(64)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64, 64, 3, 3)\n",
      "      %18 : Float(64)\n",
      "      %19 : Float(64)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64, 64, 3, 3)\n",
      "      %23 : Float(64)\n",
      "      %24 : Float(64)\n",
      "      %25 : Float(64)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64, 64, 3, 3)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Float(64)\n",
      "      %31 : Float(64)\n",
      "      %32 : Float(64, 64, 3, 3)\n",
      "      %33 : Float(64)\n",
      "      %34 : Float(64)\n",
      "      %35 : Float(64)\n",
      "      %36 : Float(64)\n",
      "      %37 : Float(128, 64, 3, 3)\n",
      "      %38 : Float(128)\n",
      "      %39 : Float(128)\n",
      "      %40 : Float(128)\n",
      "      %41 : Float(128)\n",
      "      %42 : Float(128, 128, 3, 3)\n",
      "      %43 : Float(128)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128, 64, 1, 1)\n",
      "      %48 : Float(128)\n",
      "      %49 : Float(128)\n",
      "      %50 : Float(128)\n",
      "      %51 : Float(128)\n",
      "      %52 : Float(128, 128, 3, 3)\n",
      "      %53 : Float(128)\n",
      "      %54 : Float(128)\n",
      "      %55 : Float(128)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128, 128, 3, 3)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Float(128)\n",
      "      %61 : Float(128)\n",
      "      %62 : Float(128, 128, 3, 3)\n",
      "      %63 : Float(128)\n",
      "      %64 : Float(128)\n",
      "      %65 : Float(128)\n",
      "      %66 : Float(128)\n",
      "      %67 : Float(128, 128, 3, 3)\n",
      "      %68 : Float(128)\n",
      "      %69 : Float(128)\n",
      "      %70 : Float(128)\n",
      "      %71 : Float(128)\n",
      "      %72 : Float(128, 128, 3, 3)\n",
      "      %73 : Float(128)\n",
      "      %74 : Float(128)\n",
      "      %75 : Float(128)\n",
      "      %76 : Float(128)\n",
      "      %77 : Float(128, 128, 3, 3)\n",
      "      %78 : Float(128)\n",
      "      %79 : Float(128)\n",
      "      %80 : Float(128)\n",
      "      %81 : Float(128)\n",
      "      %82 : Float(256, 128, 3, 3)\n",
      "      %83 : Float(256)\n",
      "      %84 : Float(256)\n",
      "      %85 : Float(256)\n",
      "      %86 : Float(256)\n",
      "      %87 : Float(256, 256, 3, 3)\n",
      "      %88 : Float(256)\n",
      "      %89 : Float(256)\n",
      "      %90 : Float(256)\n",
      "      %91 : Float(256)\n",
      "      %92 : Float(256, 128, 1, 1)\n",
      "      %93 : Float(256)\n",
      "      %94 : Float(256)\n",
      "      %95 : Float(256)\n",
      "      %96 : Float(256)\n",
      "      %97 : Float(256, 256, 3, 3)\n",
      "      %98 : Float(256)\n",
      "      %99 : Float(256)\n",
      "      %100 : Float(256)\n",
      "      %101 : Float(256)\n",
      "      %102 : Float(256, 256, 3, 3)\n",
      "      %103 : Float(256)\n",
      "      %104 : Float(256)\n",
      "      %105 : Float(256)\n",
      "      %106 : Float(256)\n",
      "      %107 : Float(256, 256, 3, 3)\n",
      "      %108 : Float(256)\n",
      "      %109 : Float(256)\n",
      "      %110 : Float(256)\n",
      "      %111 : Float(256)\n",
      "      %112 : Float(256, 256, 3, 3)\n",
      "      %113 : Float(256)\n",
      "      %114 : Float(256)\n",
      "      %115 : Float(256)\n",
      "      %116 : Float(256)\n",
      "      %117 : Float(256, 256, 3, 3)\n",
      "      %118 : Float(256)\n",
      "      %119 : Float(256)\n",
      "      %120 : Float(256)\n",
      "      %121 : Float(256)\n",
      "      %122 : Float(256, 256, 3, 3)\n",
      "      %123 : Float(256)\n",
      "      %124 : Float(256)\n",
      "      %125 : Float(256)\n",
      "      %126 : Float(256)\n",
      "      %127 : Float(256, 256, 3, 3)\n",
      "      %128 : Float(256)\n",
      "      %129 : Float(256)\n",
      "      %130 : Float(256)\n",
      "      %131 : Float(256)\n",
      "      %132 : Float(256, 256, 3, 3)\n",
      "      %133 : Float(256)\n",
      "      %134 : Float(256)\n",
      "      %135 : Float(256)\n",
      "      %136 : Float(256)\n",
      "      %137 : Float(256, 256, 3, 3)\n",
      "      %138 : Float(256)\n",
      "      %139 : Float(256)\n",
      "      %140 : Float(256)\n",
      "      %141 : Float(256)\n",
      "      %142 : Float(256, 256, 3, 3)\n",
      "      %143 : Float(256)\n",
      "      %144 : Float(256)\n",
      "      %145 : Float(256)\n",
      "      %146 : Float(256)\n",
      "      %147 : Float(512, 256, 3, 3)\n",
      "      %148 : Float(512)\n",
      "      %149 : Float(512)\n",
      "      %150 : Float(512)\n",
      "      %151 : Float(512)\n",
      "      %152 : Float(512, 512, 3, 3)\n",
      "      %153 : Float(512)\n",
      "      %154 : Float(512)\n",
      "      %155 : Float(512)\n",
      "      %156 : Float(512)\n",
      "      %157 : Float(512, 256, 1, 1)\n",
      "      %158 : Float(512)\n",
      "      %159 : Float(512)\n",
      "      %160 : Float(512)\n",
      "      %161 : Float(512)\n",
      "      %162 : Float(512, 512, 3, 3)\n",
      "      %163 : Float(512)\n",
      "      %164 : Float(512)\n",
      "      %165 : Float(512)\n",
      "      %166 : Float(512)\n",
      "      %167 : Float(512, 512, 3, 3)\n",
      "      %168 : Float(512)\n",
      "      %169 : Float(512)\n",
      "      %170 : Float(512)\n",
      "      %171 : Float(512)\n",
      "      %172 : Float(512, 512, 3, 3)\n",
      "      %173 : Float(512)\n",
      "      %174 : Float(512)\n",
      "      %175 : Float(512)\n",
      "      %176 : Float(512)\n",
      "      %177 : Float(512, 512, 3, 3)\n",
      "      %178 : Float(512)\n",
      "      %179 : Float(512)\n",
      "      %180 : Float(512)\n",
      "      %181 : Float(512)\n",
      "      %182 : Float(1024)\n",
      "      %183 : Float(1024)\n",
      "      %184 : Float(1024)\n",
      "      %185 : Float(1024)\n",
      "      %186 : Float(512, 1024)\n",
      "      %187 : Float(512)\n",
      "      %188 : Float(512)\n",
      "      %189 : Float(512)\n",
      "      %190 : Float(512)\n",
      "      %191 : Float(512)\n",
      "      %192 : Float(4, 512)\n",
      "      %193 : Float(4)) {\n",
      "  %195 : Float(1, 64, 112, 112) = Conv[kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3], dilations=[1, 1], group=1](%1, %2), uses = [%196.i0], scope: Sequential/Conv2d[0];\n",
      "  %197 : Float(1, 64, 112, 112) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%195, %3, %4, %5, %6), uses = [[%198.i0]], scope: Sequential/BatchNorm2d[1];\n",
      "  %198 : Float(1, 64, 112, 112) = Relu(%197), uses = [%199.i0], scope: Sequential/ReLU[2];\n",
      "  %199 : Float(1, 64, 56, 56) = MaxPool[kernel_shape=[3, 3], pads=[1, 1], strides=[2, 2]](%198), uses = [%201.i0, %209.i1], scope: Sequential/MaxPool2d[3];\n",
      "  %201 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%199, %7), uses = [%202.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %203 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%201, %8, %9, %10, %11), uses = [[%204.i0]], scope: Sequential/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %204 : Float(1, 64, 56, 56) = Relu(%203), uses = [%206.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/ReLU[relu];\n",
      "  %206 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%204, %12), uses = [%207.i0], scope: Sequential/Sequential[4]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %208 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%206, %13, %14, %15, %16), uses = [[%209.i0]], scope: Sequential/Sequential[4]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %209 : Float(1, 64, 56, 56) = Add(%208, %199), uses = [%210.i0], scope: Sequential/Sequential[4]/BasicBlock[0];\n",
      "  %210 : Float(1, 64, 56, 56) = Relu(%209), uses = [%212.i0, %220.i1], scope: Sequential/Sequential[4]/BasicBlock[0]/ReLU[relu];\n",
      "  %212 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%210, %17), uses = [%213.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %214 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%212, %18, %19, %20, %21), uses = [[%215.i0]], scope: Sequential/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %215 : Float(1, 64, 56, 56) = Relu(%214), uses = [%217.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/ReLU[relu];\n",
      "  %217 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%215, %22), uses = [%218.i0], scope: Sequential/Sequential[4]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %219 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%217, %23, %24, %25, %26), uses = [[%220.i0]], scope: Sequential/Sequential[4]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %220 : Float(1, 64, 56, 56) = Add(%219, %210), uses = [%221.i0], scope: Sequential/Sequential[4]/BasicBlock[1];\n",
      "  %221 : Float(1, 64, 56, 56) = Relu(%220), uses = [%223.i0, %231.i1], scope: Sequential/Sequential[4]/BasicBlock[1]/ReLU[relu];\n",
      "  %223 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%221, %27), uses = [%224.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %225 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%223, %28, %29, %30, %31), uses = [[%226.i0]], scope: Sequential/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %226 : Float(1, 64, 56, 56) = Relu(%225), uses = [%228.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/ReLU[relu];\n",
      "  %228 : Float(1, 64, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%226, %32), uses = [%229.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %230 : Float(1, 64, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%228, %33, %34, %35, %36), uses = [[%231.i0]], scope: Sequential/Sequential[4]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %231 : Float(1, 64, 56, 56) = Add(%230, %221), uses = [%232.i0], scope: Sequential/Sequential[4]/BasicBlock[2];\n",
      "  %232 : Float(1, 64, 56, 56) = Relu(%231), uses = [%234.i0, %243.i0], scope: Sequential/Sequential[4]/BasicBlock[2]/ReLU[relu];\n",
      "  %234 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%232, %37), uses = [%235.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %236 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%234, %38, %39, %40, %41), uses = [[%237.i0]], scope: Sequential/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %237 : Float(1, 128, 28, 28) = Relu(%236), uses = [%239.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/ReLU[relu];\n",
      "  %239 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%237, %42), uses = [%240.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %241 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%239, %43, %44, %45, %46), uses = [[%246.i0]], scope: Sequential/Sequential[5]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %243 : Float(1, 128, 28, 28) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%232, %47), uses = [%244.i0], scope: Sequential/Sequential[5]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %245 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%243, %48, %49, %50, %51), uses = [[%246.i1]], scope: Sequential/Sequential[5]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %246 : Float(1, 128, 28, 28) = Add(%241, %245), uses = [%247.i0], scope: Sequential/Sequential[5]/BasicBlock[0];\n",
      "  %247 : Float(1, 128, 28, 28) = Relu(%246), uses = [%249.i0, %257.i1], scope: Sequential/Sequential[5]/BasicBlock[0]/ReLU[relu];\n",
      "  %249 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%247, %52), uses = [%250.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %251 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%249, %53, %54, %55, %56), uses = [[%252.i0]], scope: Sequential/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %252 : Float(1, 128, 28, 28) = Relu(%251), uses = [%254.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/ReLU[relu];\n",
      "  %254 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%252, %57), uses = [%255.i0], scope: Sequential/Sequential[5]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %256 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%254, %58, %59, %60, %61), uses = [[%257.i0]], scope: Sequential/Sequential[5]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %257 : Float(1, 128, 28, 28) = Add(%256, %247), uses = [%258.i0], scope: Sequential/Sequential[5]/BasicBlock[1];\n",
      "  %258 : Float(1, 128, 28, 28) = Relu(%257), uses = [%260.i0, %268.i1], scope: Sequential/Sequential[5]/BasicBlock[1]/ReLU[relu];\n",
      "  %260 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%258, %62), uses = [%261.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %262 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%260, %63, %64, %65, %66), uses = [[%263.i0]], scope: Sequential/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %263 : Float(1, 128, 28, 28) = Relu(%262), uses = [%265.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/ReLU[relu];\n",
      "  %265 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%263, %67), uses = [%266.i0], scope: Sequential/Sequential[5]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %267 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%265, %68, %69, %70, %71), uses = [[%268.i0]], scope: Sequential/Sequential[5]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %268 : Float(1, 128, 28, 28) = Add(%267, %258), uses = [%269.i0], scope: Sequential/Sequential[5]/BasicBlock[2];\n",
      "  %269 : Float(1, 128, 28, 28) = Relu(%268), uses = [%271.i0, %279.i1], scope: Sequential/Sequential[5]/BasicBlock[2]/ReLU[relu];\n",
      "  %271 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%269, %72), uses = [%272.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/Conv2d[conv1];\n",
      "  %273 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%271, %73, %74, %75, %76), uses = [[%274.i0]], scope: Sequential/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn1];\n",
      "  %274 : Float(1, 128, 28, 28) = Relu(%273), uses = [%276.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/ReLU[relu];\n",
      "  %276 : Float(1, 128, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%274, %77), uses = [%277.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/Conv2d[conv2];\n",
      "  %278 : Float(1, 128, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%276, %78, %79, %80, %81), uses = [[%279.i0]], scope: Sequential/Sequential[5]/BasicBlock[3]/BatchNorm2d[bn2];\n",
      "  %279 : Float(1, 128, 28, 28) = Add(%278, %269), uses = [%280.i0], scope: Sequential/Sequential[5]/BasicBlock[3];\n",
      "  %280 : Float(1, 128, 28, 28) = Relu(%279), uses = [%282.i0, %291.i0], scope: Sequential/Sequential[5]/BasicBlock[3]/ReLU[relu];\n",
      "  %282 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%280, %82), uses = [%283.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %284 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%282, %83, %84, %85, %86), uses = [[%285.i0]], scope: Sequential/Sequential[6]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %285 : Float(1, 256, 14, 14) = Relu(%284), uses = [%287.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/ReLU[relu];\n",
      "  %287 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%285, %87), uses = [%288.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %289 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%287, %88, %89, %90, %91), uses = [[%294.i0]], scope: Sequential/Sequential[6]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %291 : Float(1, 256, 14, 14) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%280, %92), uses = [%292.i0], scope: Sequential/Sequential[6]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %293 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%291, %93, %94, %95, %96), uses = [[%294.i1]], scope: Sequential/Sequential[6]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %294 : Float(1, 256, 14, 14) = Add(%289, %293), uses = [%295.i0], scope: Sequential/Sequential[6]/BasicBlock[0];\n",
      "  %295 : Float(1, 256, 14, 14) = Relu(%294), uses = [%297.i0, %305.i1], scope: Sequential/Sequential[6]/BasicBlock[0]/ReLU[relu];\n",
      "  %297 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%295, %97), uses = [%298.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %299 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%297, %98, %99, %100, %101), uses = [[%300.i0]], scope: Sequential/Sequential[6]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %300 : Float(1, 256, 14, 14) = Relu(%299), uses = [%302.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/ReLU[relu];\n",
      "  %302 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%300, %102), uses = [%303.i0], scope: Sequential/Sequential[6]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %304 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%302, %103, %104, %105, %106), uses = [[%305.i0]], scope: Sequential/Sequential[6]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %305 : Float(1, 256, 14, 14) = Add(%304, %295), uses = [%306.i0], scope: Sequential/Sequential[6]/BasicBlock[1];\n",
      "  %306 : Float(1, 256, 14, 14) = Relu(%305), uses = [%308.i0, %316.i1], scope: Sequential/Sequential[6]/BasicBlock[1]/ReLU[relu];\n",
      "  %308 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%306, %107), uses = [%309.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %310 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%308, %108, %109, %110, %111), uses = [[%311.i0]], scope: Sequential/Sequential[6]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %311 : Float(1, 256, 14, 14) = Relu(%310), uses = [%313.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/ReLU[relu];\n",
      "  %313 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%311, %112), uses = [%314.i0], scope: Sequential/Sequential[6]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %315 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%313, %113, %114, %115, %116), uses = [[%316.i0]], scope: Sequential/Sequential[6]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %316 : Float(1, 256, 14, 14) = Add(%315, %306), uses = [%317.i0], scope: Sequential/Sequential[6]/BasicBlock[2];\n",
      "  %317 : Float(1, 256, 14, 14) = Relu(%316), uses = [%319.i0, %327.i1], scope: Sequential/Sequential[6]/BasicBlock[2]/ReLU[relu];\n",
      "  %319 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%317, %117), uses = [%320.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/Conv2d[conv1];\n",
      "  %321 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%319, %118, %119, %120, %121), uses = [[%322.i0]], scope: Sequential/Sequential[6]/BasicBlock[3]/BatchNorm2d[bn1];\n",
      "  %322 : Float(1, 256, 14, 14) = Relu(%321), uses = [%324.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/ReLU[relu];\n",
      "  %324 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%322, %122), uses = [%325.i0], scope: Sequential/Sequential[6]/BasicBlock[3]/Conv2d[conv2];\n",
      "  %326 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%324, %123, %124, %125, %126), uses = [[%327.i0]], scope: Sequential/Sequential[6]/BasicBlock[3]/BatchNorm2d[bn2];\n",
      "  %327 : Float(1, 256, 14, 14) = Add(%326, %317), uses = [%328.i0], scope: Sequential/Sequential[6]/BasicBlock[3];\n",
      "  %328 : Float(1, 256, 14, 14) = Relu(%327), uses = [%330.i0, %338.i1], scope: Sequential/Sequential[6]/BasicBlock[3]/ReLU[relu];\n",
      "  %330 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%328, %127), uses = [%331.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/Conv2d[conv1];\n",
      "  %332 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%330, %128, %129, %130, %131), uses = [[%333.i0]], scope: Sequential/Sequential[6]/BasicBlock[4]/BatchNorm2d[bn1];\n",
      "  %333 : Float(1, 256, 14, 14) = Relu(%332), uses = [%335.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/ReLU[relu];\n",
      "  %335 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%333, %132), uses = [%336.i0], scope: Sequential/Sequential[6]/BasicBlock[4]/Conv2d[conv2];\n",
      "  %337 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%335, %133, %134, %135, %136), uses = [[%338.i0]], scope: Sequential/Sequential[6]/BasicBlock[4]/BatchNorm2d[bn2];\n",
      "  %338 : Float(1, 256, 14, 14) = Add(%337, %328), uses = [%339.i0], scope: Sequential/Sequential[6]/BasicBlock[4];\n",
      "  %339 : Float(1, 256, 14, 14) = Relu(%338), uses = [%341.i0, %349.i1], scope: Sequential/Sequential[6]/BasicBlock[4]/ReLU[relu];\n",
      "  %341 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%339, %137), uses = [%342.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/Conv2d[conv1];\n",
      "  %343 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%341, %138, %139, %140, %141), uses = [[%344.i0]], scope: Sequential/Sequential[6]/BasicBlock[5]/BatchNorm2d[bn1];\n",
      "  %344 : Float(1, 256, 14, 14) = Relu(%343), uses = [%346.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/ReLU[relu];\n",
      "  %346 : Float(1, 256, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%344, %142), uses = [%347.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/Conv2d[conv2];\n",
      "  %348 : Float(1, 256, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%346, %143, %144, %145, %146), uses = [[%349.i0]], scope: Sequential/Sequential[6]/BasicBlock[5]/BatchNorm2d[bn2];\n",
      "  %349 : Float(1, 256, 14, 14) = Add(%348, %339), uses = [%350.i0], scope: Sequential/Sequential[6]/BasicBlock[5];\n",
      "  %350 : Float(1, 256, 14, 14) = Relu(%349), uses = [%352.i0, %361.i0], scope: Sequential/Sequential[6]/BasicBlock[5]/ReLU[relu];\n",
      "  %352 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%350, %147), uses = [%353.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Conv2d[conv1];\n",
      "  %354 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%352, %148, %149, %150, %151), uses = [[%355.i0]], scope: Sequential/Sequential[7]/BasicBlock[0]/BatchNorm2d[bn1];\n",
      "  %355 : Float(1, 512, 7, 7) = Relu(%354), uses = [%357.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/ReLU[relu];\n",
      "  %357 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%355, %152), uses = [%358.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Conv2d[conv2];\n",
      "  %359 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%357, %153, %154, %155, %156), uses = [[%364.i0]], scope: Sequential/Sequential[7]/BasicBlock[0]/BatchNorm2d[bn2];\n",
      "  %361 : Float(1, 512, 7, 7) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%350, %157), uses = [%362.i0], scope: Sequential/Sequential[7]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n",
      "  %363 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%361, %158, %159, %160, %161), uses = [[%364.i1]], scope: Sequential/Sequential[7]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n",
      "  %364 : Float(1, 512, 7, 7) = Add(%359, %363), uses = [%365.i0], scope: Sequential/Sequential[7]/BasicBlock[0];\n",
      "  %365 : Float(1, 512, 7, 7) = Relu(%364), uses = [%367.i0, %375.i1], scope: Sequential/Sequential[7]/BasicBlock[0]/ReLU[relu];\n",
      "  %367 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%365, %162), uses = [%368.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/Conv2d[conv1];\n",
      "  %369 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%367, %163, %164, %165, %166), uses = [[%370.i0]], scope: Sequential/Sequential[7]/BasicBlock[1]/BatchNorm2d[bn1];\n",
      "  %370 : Float(1, 512, 7, 7) = Relu(%369), uses = [%372.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/ReLU[relu];\n",
      "  %372 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%370, %167), uses = [%373.i0], scope: Sequential/Sequential[7]/BasicBlock[1]/Conv2d[conv2];\n",
      "  %374 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%372, %168, %169, %170, %171), uses = [[%375.i0]], scope: Sequential/Sequential[7]/BasicBlock[1]/BatchNorm2d[bn2];\n",
      "  %375 : Float(1, 512, 7, 7) = Add(%374, %365), uses = [%376.i0], scope: Sequential/Sequential[7]/BasicBlock[1];\n",
      "  %376 : Float(1, 512, 7, 7) = Relu(%375), uses = [%378.i0, %386.i1], scope: Sequential/Sequential[7]/BasicBlock[1]/ReLU[relu];\n",
      "  %378 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%376, %172), uses = [%379.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/Conv2d[conv1];\n",
      "  %380 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%378, %173, %174, %175, %176), uses = [[%381.i0]], scope: Sequential/Sequential[7]/BasicBlock[2]/BatchNorm2d[bn1];\n",
      "  %381 : Float(1, 512, 7, 7) = Relu(%380), uses = [%383.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/ReLU[relu];\n",
      "  %383 : Float(1, 512, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%381, %177), uses = [%384.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/Conv2d[conv2];\n",
      "  %385 : Float(1, 512, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%383, %178, %179, %180, %181), uses = [[%386.i0]], scope: Sequential/Sequential[7]/BasicBlock[2]/BatchNorm2d[bn2];\n",
      "  %386 : Float(1, 512, 7, 7) = Add(%385, %376), uses = [%387.i0], scope: Sequential/Sequential[7]/BasicBlock[2];\n",
      "  %387 : Float(1, 512, 7, 7) = Relu(%386), uses = [%388.i0, %389.i0], scope: Sequential/Sequential[7]/BasicBlock[2]/ReLU[relu];\n",
      "  %388 : Float(1, 512, 1, 1) = MaxPool[kernel_shape=[7, 7], pads=[0, 0], strides=[7, 7]](%387), uses = [%390.i0], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %389 : Float(1, 512, 1, 1) = AveragePool[kernel_shape=[7, 7], pads=[0, 0], strides=[7, 7]](%387), uses = [%390.i1], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %390 : Float(1, 1024, 1, 1) = Concat[axis=1](%388, %389), uses = [%391.i0], scope: Sequential/AdaptiveConcatPool2d[8];\n",
      "  %391 : Float(1, 1024) = Reshape[shape=[1, -1]](%390), uses = [%392.i0], scope: Sequential/Flatten[9];\n",
      "  %392 : Float(1, 1024, 1, 1) = Reshape[shape=[1, 1024, 1, 1]](%391), uses = [%393.i0], scope: Sequential/BatchNormExpand[10];\n",
      "  %394 : Float(1, 1024, 1, 1) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%392, %182, %183, %184, %185), uses = [[%395.i0]], scope: Sequential/BatchNorm1d[11];\n",
      "  %395 : Float(1, 1024) = Reshape[shape=[1, 1024]](%394), uses = [%396.i0], scope: Sequential/BatchNormContract[12];\n",
      "  %397 : Float(1, 1024), %398 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.25](%395), uses = [[%401.i0], []], scope: Sequential/Dropout[13];\n",
      "  %401 : Float(1, 512) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%397, %186, %187), uses = [%402.i0], scope: Sequential/Linear[14];\n",
      "  %402 : Float(1, 512) = Relu(%401), uses = [%403.i0], scope: Sequential/ReLU[15];\n",
      "  %403 : Float(1, 512, 1, 1) = Reshape[shape=[1, 512, 1, 1]](%402), uses = [%404.i0], scope: Sequential/BatchNormExpand[16];\n",
      "  %405 : Float(1, 512, 1, 1) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%403, %188, %189, %190, %191), uses = [[%406.i0]], scope: Sequential/BatchNorm1d[17];\n",
      "  %406 : Float(1, 512) = Reshape[shape=[1, 512]](%405), uses = [%407.i0], scope: Sequential/BatchNormContract[18];\n",
      "  %408 : Float(1, 512), %409 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%406), uses = [[%412.i0], []], scope: Sequential/Dropout[19];\n",
      "  %412 : Float(1, 4) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%408, %192, %193), uses = [%413.i0], scope: Sequential/Linear[20];\n",
      "  %413 : Float(1, 4) = Softmax[axis=1](%412), uses = [%414.i0], scope: Sequential/LogSoftmax[21];\n",
      "  %414 : Float(1, 4) = Log(%413), uses = [%0.i0], scope: Sequential/LogSoftmax[21];\n",
      "  return (%414);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "\n",
    "# the first dimension of the dummy_variable is the batch_size that will be used in the exported model\n",
    "dummy_input = Variable(torch.randn(1, 3, 224, 224)).cpu()\n",
    "\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"sleeves_fixed_batchnorm.proto\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
